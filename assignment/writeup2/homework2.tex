\documentclass[11pt]{article}

\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{url}
\usepackage{wrapfig}
\usepackage{color}
\usepackage{marvosym}
\usepackage{enumerate}
\usepackage{subfigure}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref} 


\oddsidemargin 0mm
\evensidemargin 5mm
\topmargin -20mm
\textheight 240mm
\textwidth 160mm




\newcommand{\vw}{{\bf w}}
\newcommand{\vx}{{\bf x}}
\newcommand{\vy}{{\bf y}}
\newcommand{\vxi}{{\bf x}_i}
\newcommand{\yi}{y_i}
\newcommand{\vxj}{{\bf x}_j}
\newcommand{\vxn}{{\bf x}_n}
\newcommand{\yj}{y_j}
\newcommand{\ai}{\alpha_i}
\newcommand{\aj}{\alpha_j}
\newcommand{\X}{{\bf X}}
\newcommand{\Y}{{\bf Y}}
\newcommand{\vz}{{\bf z}}
\newcommand{\msigma}{{\bf \Sigma}}
\newcommand{\vmu}{{\bf \mu}}
\newcommand{\vmuk}{{\bf \mu}_k}
\newcommand{\msigmak}{{\bf \Sigma}_k}
\newcommand{\vmuj}{{\bf \mu}_j}
\newcommand{\msigmaj}{{\bf \Sigma}_j}
\newcommand{\pij}{\pi_j}
\newcommand{\pik}{\pi_k}
\newcommand{\D}{\mathcal{D}}
\newcommand{\el}{\mathcal{L}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\vxij}{{\bf x}_{ij}}
\newcommand{\vt}{{\bf t}}
\newcommand{\yh}{\hat{y}}
\newcommand{\code}[1]{{\footnotesize \tt #1}}
\newcommand{\alphai}{\alpha_i}

\pagestyle{myheadings} 
\markboth{Homework 2}{Spring 2014 CS 475 Machine Learning: Homework 2}


\title{CS 475 Machine Learning: Homework 2\\Supervised Classifiers 1\\
\Large{Due: Wednesday February 19, 2014, 12:00pm}\\
100 Points Total \hspace{1cm} Version 1.0}
\author{Yiran Zhang}
\date{}

\begin{document}
\large
\maketitle
\thispagestyle{headings}

\vspace{-.5in}

\section{Analytical (50 points)}

\paragraph{1) Decision Tree and Logistic Regression (8 points)}
Consider a binary classification task (label $y$) with four features ($x$):

\begin{tabular}{ |l|l|l|l|l| }
\hline
$x_1$ & $x_2$ & $x_3$ & $x_4$ & $y$ \\
\hline
 0& 1 & 1& -1 & 1 \\
 0&  1 & 1& 1 & 0 \\
 0&  -1 & 1& 1 & 1 \\
 0&  -1 & 1& -1 & 0 \\
\hline
\end{tabular}

\begin{enumerate}[(a)]
\item Can this function be learned using a decision tree? If so, provide such a tree (describe each node in the tree). If not, prove it.


This cannot be learned by a decision tree. Since a decision tree will look greedily for the attribute that is going to bring the most information gain but none of the four attributes has positive IG.
\begin{eqnarray*}
IG(y | x_2) &=&H(Y) - H(Y|X_2) \\
		 &=&1 - \sum_{x_2,y} p(x_2, y) \log \frac{p(x_2,y)}{p(x_2)}\\
		 &=&1 -4 \cdot 0.25 \cdot \log 0.5 \\
		 &=&0
\end{eqnarray*}

\begin{eqnarray*}
IG(y | x_4) &=&H(Y) - H(Y|X_4) \\
		 &=&1 - \sum_{x_4,y} p(x_4, y) \log \frac{p(x_4,y)}{p(x_4)}\\
		 &=&1 -4 \cdot 0.25 \cdot \log 0.5 \\
		 &=&0
\end{eqnarray*}

Also for $x_1$ and $x_3$, they are the same for all y outcome, so they are providing any information.
\item Can this function be learned using a logistic regression classifier? If yes, give some example parameter weights. If not, why not.



\item For the models above where you can learn this function, the learned model may over-fit the data. Propose a solution for each model on how to avoid over-fitting.
\end{enumerate}

\paragraph{2) Stochastic Gradient Descent (8 points)}
In the programming part of this assignment you implemented Gradient Descent. A stochastic variation of that method (Stochastic Gradient Descent) takes an estimate of the gradient based on a single sampled example, and takes a step based on that gradient. This process is repeated many times until convergence. To summarize:
\begin{enumerate}
\item Gradient descent: compute the gradient over all the training examples, take a gradient step, repeat until convergence.
\item Stochastic gradient descent: sample a single training example, compute the gradient over that training example, take a gradient step, repeat until convergence.
\end{enumerate}

In the limit, will both of these algorithms converge to the same optimum or different optimum? Answer this question for both convex and non-convex functions. Prove your answers.

\paragraph{3) Regularizer of Regression (10 points)}
In linear regression we want to minimize the sum of square loss
\begin{align}
\hat{\beta}=\arg\!\min\limits_{\beta}||y-X\beta||_2^2
\end{align}
To address overfitting, we might plug in a regularizer $||\beta||_q$ as:
\begin{align}
\hat{\beta}=\arg\!\min\limits_{\beta}||y-X\beta||_2^2+\lambda||\beta||_q
\end{align}
where $||\cdot||_q$ is the q-norm and $\lambda$ is the regularization parameter
\begin{enumerate}[(a)]
\item
What is the effect on $\beta$ when $q=0$, $q=1$ and $q=2$? Explain why this is the case.
\item
What is the effect of $\lambda$ in terms of variance/bias trade-off? How do we usually select a proper $\lambda$?
\item
Suppose each example has three features and the corresponding parameters are $\beta_0,\beta_1$ and $\beta_2$. If we formulate the regularizer as $||\beta_{\{0,1\}}||_2+|\beta_2|$, where $\beta_{\{0,1\}}$ is a 2 dimensional vector containing the first two elements of $\beta$. Describe the effect of this regularizer.
\end{enumerate}


\paragraph{4) Constructing Generalized linear models. (12 points)}
Generalized linear models (GLMs), especially logistic regression are heavily used by banks, credit card companies and insurance companies. Actually, when you apply for a credit card, banks may put your information into a logistic regression model to decide whether you are eligible.
\begin{enumerate}[(a)]
\item The GLMs are closely related to the exponential distribution family, which has the probability density/mass function $f(y; \theta)$ in the form
\begin{align}
f(y; \theta) = h(y)e^{\eta(\theta)\cdot T(y)-A(\theta)},
\end{align}
where $h,\eta,T,A$ are some known functions.

Consider a classification or regression problem where we would like to predict the value of some random variable $y$ as a function of
$x$. To derive a GLM for this problem, we will make the following three assumptions about the conditional distribution of $y$ given $x$ and about our model:
\begin{enumerate}[1.]
\item
$y | x; \theta \sim ExponentialFamily(\eta)$. I.e., given $x$ and $\theta$, the distribution of $y$ follows some exponential family distribution, with parameter $\eta$.
\item
Given $x$, our goal is to predict the expected value of $T(y)$ given $x$. In most of our examples, we will have $T(y) = y$, so this means we
would like the prediction $h(x)$ output by our learned hypothesis $h$ to satisfy $h(x) = E[y|x]$.
\item
The natural parameter $\eta$ and the inputs $x$ are related linearly: $\eta = \theta^T x$
\end{enumerate}
Derive the expression of logistic regression from the Bernoulli distribution:
\begin{align}
h_{\theta}(x)=\frac{1}{1+\exp(-\theta^Tx)}
\end{align}
by following the above three assumptions.

Proof:
\begin{align}
f(y; \theta) = h(y)e^{\eta(\theta)\cdot T(y)-A(\theta)},
\end{align}

\item
The GLMs often contain some transformation, which is non-linear such as the log-odds-ratio transformation in the logistic regression. Why do we still call them ``linear''?
\end{enumerate}


\paragraph{5) Convex Optimization (12 points)}
Jenny at Acme Inc. is working hard on her new machine learning algorithm. She starts by writing an objective function
that captures her thoughts about the problem. However, after writing the program that optimizes the objective
and getting poor results, she returns to the objective function in frustration. Turning to her colleague Matilda,
who took CS 475 at Johns Hopkins, she asks for advice. ``Have you checked that your function is convex?'' asks Matilda.
``How?'' asks Jenny.
\begin{enumerate}[(a)]
\item Jenny's function can be written as $f(g(x))$, where $f(x)$ and $g(x)$ are convex. Prove that $f(g(x))$ is a convex function. (Hint: You may find it helpful to use the definition of convexity. Do not use gradient or Hessian, since $f$ and $g$ may not have them.)

Proof:

\begin{eqnarray*}
f(g(\lambda x_1 + (1-\lambda) x_2)) &\le& f(\lambda g(x_1) + (1 - \lambda) g(x_2))\\
&\le& \lambda f(g(x_1)) + (1 - \lambda) f(g(x_2))
\end{eqnarray*}

The first equality is because monotonicity and the convexity of g. The second inequality is because of the convexity.

\item Jenny realizes that she made an error and that her function is instead
$f(x)-g(x)$, where $f(x)$ and $g(x)$ are convex functions. Her objective may or may not be convex. Give examples of functions $f(x)$ and $g(x)$ whose difference is convex, and functions $\bar{f}(x)$ and $\bar{g}(x)$ whose difference is non-convex.

$$f(x) = x, g(x) =  0; \bar f(x) = 0 \bar g(x) = x^2 $$
\end{enumerate}
``I now know that my function is non-convex,'' Jenny says, ``but why does that matter?''
\begin{enumerate}[(a)]
\setcounter{enumi}{2}
\item Why was Jenny getting poor results with a non-convex function?

Using gradient decent on non-convex function can result in convergence to a local optimum. When the function is not that smooth and has a lot of small hills and bowls everywhere, it's very likely that we are going to reach a local minimum, which is much worse than the global one.

\item One approach for convex optimization is to iteratively compute a descent direction and take a step along that direction to have a new value of the parameters. The choice of a proper stepsize is not so trivial. In gradient descent algorithm, the stepsize is chosen such that it is proportional to the magnitude of the gradient at the current point. What might be the problem if we fix the stepsize to a constant regardless of the current gradient? Discuss when stepsize is too small or too large.

It might happen that we are bouncing around the optimal point but not reach it. Think about the objective is $f(x) = |x|$ and we are at $x = 0.01$ and the step size is 0.02. When we take the gradient descent, then the current point will be bouncing between $-0.01 $ and $0.01$.
\end{enumerate}




\section{What to Submit}
In each assignment you will submit two things.
\begin{enumerate}
\item {\bf Code:} Your code as a zip file named {\tt library.zip}. {\bf You must submit source code (.java files)}. We will run your code using the exact command lines described above, so make sure it works ahead of time. Remember to submit all of the source code, including what we have provided to you.
\item {\bf Writeup:} Your writeup as a {\bf PDF file} (compiled from latex) containing answers to the analytical questions asked in the assignment. Make sure to include your name in the writeup PDF and use the provided latex template for your answers.
\end{enumerate}
Make sure you name each of the files exactly as specified (library.zip and writeup.pdf).

To submit your assignment, visit the ``Homework'' section of the website (\href{http://www.cs475.org/}{http://www.cs475.org/}.)

\section{Questions?}
Remember to submit questions about the assignment to the appropriate group on the class discussion board: \href{http://bb.cs475.org/}{http://bb.cs475.org}.

\end{document}
